{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resonance model train+test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pydub import AudioSegment\n",
    "#trim audio and save\n",
    "import numpy\n",
    "import math\n",
    "import scipy.io.wavfile\n",
    "\n",
    "#gneral utils\n",
    "formats_to_convert = ['.m4a']\n",
    "def trans_to_wav(dirpath):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dirpath):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(tuple(formats_to_convert)):\n",
    "\n",
    "                filepath = dirpath + '/' + filename\n",
    "                (path, file_extension) = os.path.splitext(filepath)\n",
    "                file_extension_final = file_extension.replace('.', '')\n",
    "                try:\n",
    "                    track = AudioSegment.from_file(filepath,\n",
    "                            file_extension_final)\n",
    "                    wav_filename = filename.replace(file_extension_final, 'wav')\n",
    "                    wav_path = dirpath + '/' + wav_filename\n",
    "                    print('CONVERTING: ' + str(filepath))\n",
    "                    file_handle = track.export(wav_path, format='wav')\n",
    "                    os.remove(filepath)\n",
    "                except:\n",
    "                    print(\"ERROR CONVERTING \" + str(filepath))\n",
    "\n",
    "def cut_audio_save(filepath,start,end,new_name):\n",
    "    sound = AudioSegment.from_file(filepath)\n",
    "    demo = sound[start:end]#in ms\n",
    "    demo.export(new_name+\".wav\", format=\"wav\")\n",
    "    \n",
    "    \n",
    "dirpath1=r\"C:\\Users\\gaoyu\\OneDrive\\Documents\\Sound recordings\"\n",
    "dirpath2=r\"C:\\Users\\gaoyu\\Desktop\\data_resonance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature utils\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import python_speech_features as mfcc\n",
    "\n",
    " \n",
    "def calculate_delta(array):\n",
    "    \"\"\"Calculate and returns the delta of given feature vector matrix\"\"\"\n",
    "    rows,cols = array.shape\n",
    "    deltas = np.zeros((rows,20))\n",
    "    N = 2\n",
    "    for i in range(rows):\n",
    "        index = []\n",
    "        j = 1\n",
    "        while j <= N:\n",
    "            if i-j==rows-1:#pay attenstion here original text i-j rows-1\n",
    "                second = rows-1\n",
    "            else:\n",
    "                second = i+j\n",
    "            index.append((second,first))\n",
    "            j+=1\n",
    "        deltas[i] = ( array[index[0][0]]-array[index[0][1]] + (2 * (array[index[1][0]]-array[index[1][1]])) ) / 10\n",
    "    return deltas\n",
    " \n",
    "def extract_features(audio,rate):\n",
    "    \"\"\"extract 20 dim mfcc features from an audio, performs CMS and combines\n",
    "    delta to make it 40 dim feature vector\"\"\"   \n",
    "\n",
    "    mfcc_feat = mfcc.mfcc(audio,rate, 0.025, 0.01,20,appendEnergy = True)\n",
    "    mfcc_feat = preprocessing.scale(mfcc_feat)\n",
    "    #delta = calculate_delta(mfcc_feat)\n",
    "    #combined = np.hstack((mfcc_feat,delta))\n",
    "    combined = np.vstack((mfcc_feat,mfcc_feat))\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9929\n",
      "49\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn.mixture import GMM\n",
    "import warnings\n",
    "#file setup\n",
    "#0.660694-4.068803-6.604524-10 (0.2 persample)\n",
    "#no resonance-forward-backward-nasal\n",
    "file_to_cut=r\"C:\\Users\\gaoyu\\Desktop\\music try\\data_resonance\\all_types_F_chain.wav\"\n",
    "folder_to_save=\"resonance_set\\\\\" \n",
    "#cut_audio_save(filepath,start,end,new_name)\n",
    "def prep_data(filepath,dirpath):\n",
    "    sound = AudioSegment.from_file(filepath)\n",
    "    length=len(sound)#9929ms \n",
    "    duration=200\n",
    "    intervals=length//duration\n",
    "    L=[]\n",
    "    \n",
    "    for i in range(intervals):\n",
    "        start=i*duration\n",
    "        if i==intervals-1:\n",
    "            end=length\n",
    "        else:\n",
    "            end=(i+1)*duration\n",
    "        demo = sound[start:end]#in ms\n",
    "        #name set\n",
    "        if end<661:\n",
    "            new_name=\"no\"\n",
    "        elif end<4061:\n",
    "            new_name=\"forward\"\n",
    "        elif end<6604:\n",
    "            new_name=\"backward\"\n",
    "        else:\n",
    "            new_name=\"nasal\"\n",
    "        file_name=new_name+\"-\"+str(i)+\".wav\"\n",
    "        L.append(file_name+\"\\n\")\n",
    "        demo.export(dirpath+file_name, format=\"wav\")\n",
    "    \n",
    "    file1 = open(\"development_set_enroll.txt\",\"w\") \n",
    "    file1.writelines(L) \n",
    "    file1.close()\n",
    "    \n",
    "    \n",
    "prep_data(file_to_cut,folder_to_save)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ modeling completed for resonance type: no.gmm  with data point =  (234, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ modeling completed for resonance type: forward.gmm  with data point =  (1326, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ modeling completed for resonance type: backward.gmm  with data point =  (1014, 20)\n",
      "+ modeling completed for resonance type: nasal.gmm  with data point =  (1300, 20)\n"
     ]
    }
   ],
   "source": [
    "#GMM part\n",
    "#model training\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn.mixture import GMM\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "#path to training data\n",
    "source   = \"resonance_set\\\\\"  \n",
    " \n",
    "#path where training speakers will be saved\n",
    "dest = \"resonance_models\\\\\"\n",
    "train_file = r\"C:\\Users\\gaoyu\\Desktop\\music try\\data_resonance\\development_set_enroll.txt\"\n",
    "file_paths = open(train_file,'r')\n",
    " \n",
    "#path of file to train=\"resonancetype-number\"    \n",
    "count = 1\n",
    "# Extracting features for each resonance (5 files per resonance styles)\n",
    "features = np.asarray(())\n",
    "for path in file_paths:\n",
    "    path = path.strip() \n",
    "    # read the audio\n",
    "    sr,audio = read(source + path)\n",
    "    vector = extract_features(audio,sr)\n",
    "    if features.size == 0:\n",
    "        features = vector\n",
    "    else:\n",
    "        try:\n",
    "            features = np.vstack((features, vector))\n",
    "        except:\n",
    "            pass\n",
    "    # when features of 5 files of speaker are concatenated, then do model training\n",
    "    if count == 3:\n",
    "        gmm = GMM(n_components = 3, n_iter = 200, covariance_type='diag',n_init = 3)\n",
    "        gmm.fit(features)\n",
    " \n",
    "        # dumping the trained gaussian model\n",
    "        picklefile = path.split(\"-\")[0]+\".gmm\"\n",
    "        cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "        print ('+ modeling completed for resonance type:',picklefile,\" with data point = \",features.shape)\n",
    "        features = np.asarray(())\n",
    "        #count = 0\n",
    "    elif count == 20:\n",
    "        gmm = GMM(n_components = 20-3, n_iter = 200, covariance_type='diag',n_init = 3)\n",
    "        gmm.fit(features)\n",
    " \n",
    "        # dumping the trained gaussian model\n",
    "        picklefile = path.split(\"-\")[0]+\".gmm\"\n",
    "        cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "        print ('+ modeling completed for resonance type:',picklefile,\" with data point = \",features.shape)\n",
    "        features = np.asarray(())\n",
    "        #count = 0\n",
    "    elif count == 33:\n",
    "        gmm = GMM(n_components = 33-20, n_iter = 200, covariance_type='diag',n_init = 3)\n",
    "        gmm.fit(features)\n",
    " \n",
    "        # dumping the trained gaussian model\n",
    "        picklefile = path.split(\"-\")[0]+\".gmm\"\n",
    "        cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "        print ('+ modeling completed for resonance type:',picklefile,\" with data point = \",features.shape)\n",
    "        features = np.asarray(())\n",
    "        #count = 0\n",
    "    elif count == 49:\n",
    "        \n",
    "        gmm = GMM(n_components = 15, n_iter = 200, covariance_type='diag',n_init = 3)\n",
    "        gmm.fit(features)\n",
    " \n",
    "        # dumping the trained gaussian model\n",
    "        picklefile = path.split(\"-\")[0]+\".gmm\"\n",
    "        cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "        print ('+ modeling completed for resonance type:',picklefile,\" with data point = \",features.shape)\n",
    "        features = np.asarray(())\n",
    "        #count = 0\n",
    "    count = count + 1\n",
    "   \n",
    "  \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_male_baseline.wav\n",
      "[-7433.01897468 -7338.11019522 -7377.10246851 -7521.58747878]\n",
      "1\n",
      "\tdetected as -  forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward_F4_a_male.wav\n",
      "[-25154.26943587 -25562.91923749 -25691.30938192 -25513.04952436]\n",
      "0\n",
      "\tdetected as -  backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward_lowered_larynx_male_a_F.wav\n",
      "[-27627.62109675 -29646.34552096 -28976.62233508 -28545.1753033 ]\n",
      "0\n",
      "\tdetected as -  backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_female_forward.wav\n",
      "[-16947.23970618 -17283.41270666 -17407.68523388 -17404.8676683 ]\n",
      "0\n",
      "\tdetected as -  backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_female_backward.wav\n",
      "[-30478.44812965 -31917.38921853 -32073.21002835 -30651.54357625]\n",
      "0\n",
      "\tdetected as -  backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_female_trans_for.wav\n",
      "[-7760.63953085 -7868.93703151 -7850.75846904 -7849.44534685]\n",
      "0\n",
      "\tdetected as -  backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_female_trans_back.wav\n",
      "[-14400.1742067  -15484.59127426 -15533.27017631 -14886.5683451 ]\n",
      "0\n",
      "\tdetected as -  backward\n"
     ]
    }
   ],
   "source": [
    "#test models:\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    " \n",
    "#path to training data\n",
    "source   = \"development_set\\\\\"\n",
    "modelpath = \"resonance_models\\\\\"\n",
    "\n",
    "test_file = r\"C:\\Users\\gaoyu\\Desktop\\music try\\data_resonance\\development_set_test.txt\"\n",
    "file_paths = open(test_file,'r')\n",
    "gmm_files = [os.path.join(modelpath,fname) for fname in\n",
    "              os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    " \n",
    "#Load the Gaussian gender Models\n",
    "models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "resonances   = [fname.split(\"\\\\\")[-1].split(\".gmm\")[0] for fname\n",
    "              in gmm_files] \n",
    "# Read the test directory and get the list of test audio files\n",
    "for path in file_paths:   \n",
    "    path = path.strip()\n",
    "    print(path)\n",
    "    sr,audio = read(path)\n",
    "    vector   = extract_features(audio,sr)\n",
    "    #vector=vector.reshape((1, -1))\n",
    "    log_likelihood = np.zeros(len(models)) \n",
    " \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]  #checking with each model one by one\n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum()\n",
    "\n",
    "    winner = np.argmax(log_likelihood)\n",
    "    print(winner)\n",
    "    print (\"\\tdetected as - \", resonances[winner])\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vibrato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from scipy.fftpack import fft\n",
    "import pandas\n",
    "import os\n",
    "NFFT = 512\n",
    "F_low = 5\n",
    "F_high = 8\n",
    "\n",
    "'''\n",
    "功能：提取颤音特征\n",
    "输入：time-pitch序列（间隔5ms）\n",
    "输出：颤音特征：Likelihood,Rate,Extent\n",
    "'''\n",
    "def VibratoFeature(time_pitch):\n",
    "    NFFT = 512\n",
    "    pitch_hop = 0.005\n",
    "    freq = np.arange(0, 1 / (2 * pitch_hop), (1 / (pitch_hop * NFFT)))\n",
    "\n",
    "    time = time_pitch[:,0]\n",
    "    pitch = time_pitch[:,1]\n",
    "    vib_features = np.array([])\n",
    "\n",
    "    cur_time = 0\n",
    "    time_length = 0.5 #分析长度为0.5s\n",
    "    time_hop = 0.25 #滑动为0.25秒\n",
    "    while cur_time<time[-1]:\n",
    "        time_stamp = np.array([cur_time,cur_time+time_length])\n",
    "        \n",
    "        features = []\n",
    "        n1 = np.argmin(np.abs(np.subtract(time, time_stamp[0])))\n",
    "        n2 = np.argmin(np.abs(np.subtract(time, time_stamp[1])))\n",
    "        pitch_snippet = pitch[n1:n2+1]\n",
    "        time_snippet = time[n1:n2+1]\n",
    "\n",
    "        pitch_snippet = pitch_snippet - np.mean(pitch_snippet)\n",
    "\n",
    "        ### Likelihood\n",
    "        X = np.abs(fft(pitch_snippet * np.hamming(len(pitch_snippet)), NFFT))\n",
    "        Xhalf_norm = X[0:int(NFFT / 2)] / sum(X[0:int(NFFT / 2)])\n",
    "        vib_region = Xhalf_norm[(freq >= F_low) & (freq <= F_high)]\n",
    "        vib_power = sum(vib_region)\n",
    "        vib_sum_power = sum(Xhalf_norm)\n",
    "        vib_power_ratio = vib_power/vib_sum_power\n",
    "        features.append(vib_power_ratio)\n",
    "\n",
    "        ### Rate\n",
    "        zero_crossings = np.where(np.diff(np.sign(pitch_snippet)))[0]\n",
    "        time_zerocrossings = time_snippet[zero_crossings[0::2]]\n",
    "        if len(time_zerocrossings) <=1: #不够一个周期==time_zerocrossings不到两个\n",
    "            if vib_features.size == 0:\n",
    "                vib_features = np.array([0.,0.,0.])\n",
    "            else:\n",
    "                vib_features = np.vstack((vib_features, np.array([0.,0.,0.])))\n",
    "            cur_time+=time_hop\n",
    "            continue\n",
    "        rate = 1.0/((1.0/(len(time_zerocrossings)-1))*sum(np.abs(time_zerocrossings[0:-1]-time_zerocrossings[1:])))\n",
    "        features.append(rate)\n",
    "\n",
    "        ### Extent\n",
    "        pitch_max = []\n",
    "        for index in range(len(zero_crossings)-1):\n",
    "            pitch_max.append(max(np.abs(pitch_snippet[zero_crossings[index]:zero_crossings[index+1]])))\n",
    "        pitch_max = np.array(pitch_max)\n",
    "        extent =  (0.5/(len(pitch_max)-1))*sum(np.abs(pitch_max[0:-1]+pitch_max[1:]))\n",
    "        features.append(extent)\n",
    "\n",
    "        if vib_features.size == 0:\n",
    "            vib_features = np.array(features)\n",
    "        else:\n",
    "            vib_features = np.vstack((vib_features,np.array(features)))\n",
    "        # print(features)\n",
    "        cur_time+=time_hop\n",
    "    return vib_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vibroato test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################按照文件夹遍历##############################\n",
    "# base_dir = \"G:/data/\"\n",
    "audio_name = \"27.wav\"\n",
    "audio_path = \"temp/\"+audio_name\n",
    "pitch_path = \"temp/\"+audio_name[:-4]+\"_pitch.csv\"\n",
    "pitch_path2 = \"temp/\"+audio_name[:-4]+\"_pitch_5ms.csv\"\n",
    "note_path = \"temp/\"+audio_name[:-4]+\"_note.csv\"\n",
    "#########################################################################\n",
    "\n",
    "(fs, ori_sig) = wav.read(audio_path)\n",
    "window = NFFT / (fs * 1.0)\n",
    "hop = window / 2.0\n",
    "#############################如果以前没有提取过pitch##################################\n",
    "if not os.path.exists(pitch_path2):\n",
    "    time_hop = (1000/44100)*256\n",
    "    command = \"./AudioBaseTestPitch0625 \"+audio_path+\" \"+pitch_path+\" \"+note_path+\" \"+pitch_path2\n",
    "    print(command)\n",
    "    os.system(command)\n",
    "    #########################################################################\n",
    "    #padding补0\n",
    "    data = pandas.read_csv(pitch_path2).values\n",
    "    data = data[np.lexsort(data[:,::-1].T)]\n",
    "    last_time = (len(ori_sig)/fs)*1000\n",
    "\n",
    "    #旧的pitch path 2可以删掉\n",
    "    command2 = \"rm \"+pitch_path2 \n",
    "    os.system(command2)\n",
    "\n",
    "    #以5ms为边界，重写pitch path\n",
    "    time_hop = 5\n",
    "    f = open(pitch_path2,'w')\n",
    "    cur_time = 0 #curtime为时间戳\n",
    "    j = 0 #j为遍历data的序号\n",
    "    while cur_time < last_time:\n",
    "        if j>=data.shape[0] or abs(cur_time-data[j][0])>=(time_hop/2): #以time_hop为边界\n",
    "            f.write(str(round(cur_time/1000.0,3))+\" 0\"+'\\n')\n",
    "        else:\n",
    "            f.write(str(round(data[j][0]/1000.0,3))+\" \"+str(data[j][1])+'\\n')\n",
    "            j+=1\n",
    "        cur_time+=time_hop\n",
    "    f.close()\n",
    "#############################读取pitch_time##################################\n",
    "f = open(pitch_path2, 'r')\n",
    "obj = csv.reader(f, delimiter=' ')\n",
    "cols = []\n",
    "for row in obj:\n",
    "    if len(row)<2: break\n",
    "    if np.size(cols) == 0:\n",
    "        cols = [float(elem) for elem in row]\n",
    "    else:\n",
    "        cols = np.vstack((cols, [float(elem) for elem in row]))\n",
    "f.close()\n",
    "time_pitch = cols\n",
    "############################提取颤音特征#################################\n",
    "vib_features = VibratoFeature(time_pitch)\n",
    "\n",
    "for i in vib_features:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine res+vib==>technique_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-17-15fcf27b3fb0>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-15fcf27b3fb0>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    np.vstack((vib_features,res_features)\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#To get technique vector for each file:\n",
    "\n",
    "def Resonance_feature(path,modelpath):\n",
    "    gmm_files = [os.path.join(modelpath,fname) for fname in\n",
    "    os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "    #Load the Gaussian gender Models\n",
    "    models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "\n",
    "    res_features = np.array([])\n",
    "    features = []\n",
    "    res_features=np.asarray(features)\n",
    "    #calculate log_likelihood(4,) for frame of 0.5ms and concanentate (4,frame_num)\n",
    "    sr,audio = read(path)\n",
    "    #TODO:cut the audio to sections of 0.5ms and loop to calculate each vector\n",
    "    #loop starts\n",
    "    vector = extract_features(audio,sr)\n",
    "    #vector=vector.reshape((1, -1))\n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]  #checking with each model one by one\n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum()\n",
    "    features.extend(log_likelihood)\n",
    "    #loop ends\n",
    "    res_feature=np.asarray(features)\n",
    "    return res_feature\n",
    "\n",
    "#######################################################################\n",
    "#0.get model path\n",
    "modelpath = \"resonance_models\\\\\"\n",
    "#1.get resonance feature\n",
    "res_feature=Resonance_future(path,modelpath)\n",
    "#2.get time-pitch series\n",
    "time_pitch=get_time_pitch_series(path)\n",
    "#get vib_feature\n",
    "vib_features = VibratoFeature(time_pitch)\n",
    "#stack with vibrato features(3,) for frame of 0.5ms and concanenate (3,frame_num) \n",
    "########################################################################\n",
    "#stack to get (7,frame_num)\n",
    "technique_vector=np.vstack((vib_features,res_features))\n",
    "#pass technique_vector to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
